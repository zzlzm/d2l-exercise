{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "64642667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import torchvision\n",
    "\n",
    "batch_size = 256\n",
    "def load_data(batch_size, isTrain=True):\n",
    "    dataset = torchvision.datasets.FashionMNIST(root='../data',\n",
    "                                                train=isTrain,\n",
    "                                                transform=torchvision.transforms.ToTensor())\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=isTrain)\n",
    "    for imgs, labels in data_loader:\n",
    "        yield imgs.reshape(-1, 784).type(torch.float32), labels\n",
    "# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "train_iter = load_data(batch_size, True)\n",
    "test_iter = load_data(batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "10be8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.] * n\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "    \n",
    "    def add(self, *args):\n",
    "        self.data = [a + b for (a, b) in zip(self.data, args)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64479bac",
   "metadata": {},
   "source": [
    "### 定义ReLu函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "77f84f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e53228e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = nn.Parameter(torch.randn(784, 256, requires_grad=True) * 0.01)\n",
    "b1 = nn.Parameter(torch.zeros(256, requires_grad=True))\n",
    "W2 = nn.Parameter(torch.randn(256, 10, requires_grad=True) * 0.01)\n",
    "b2 = nn.Parameter(torch.zeros(10, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "eb39c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exp_X = torch.exp(X)\n",
    "    return exp_X / exp_X.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f4f35b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape(-1, 784)\n",
    "    H = relu(X @ W1 + b1)\n",
    "    return (H @ W2 + b2)\n",
    "#     return X @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cb3bcbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "35d5097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(243.5515, grad_fn=<AddBackward0>)\n",
      "tensor(7645.) 10000.0\n",
      "tensor(141.4802, grad_fn=<AddBackward0>)\n",
      "tensor(7612.) 10000.0\n",
      "tensor(121.3977, grad_fn=<AddBackward0>)\n",
      "tensor(7907.) 10000.0\n",
      "tensor(112.3852, grad_fn=<AddBackward0>)\n",
      "tensor(8140.) 10000.0\n",
      "tensor(106.7940, grad_fn=<AddBackward0>)\n",
      "tensor(8307.) 10000.0\n",
      "tensor(102.0197, grad_fn=<AddBackward0>)\n",
      "tensor(8330.) 10000.0\n",
      "tensor(98.2086, grad_fn=<AddBackward0>)\n",
      "tensor(8236.) 10000.0\n",
      "tensor(94.8536, grad_fn=<AddBackward0>)\n",
      "tensor(8275.) 10000.0\n",
      "tensor(92.7512, grad_fn=<AddBackward0>)\n",
      "tensor(8379.) 10000.0\n",
      "tensor(89.8452, grad_fn=<AddBackward0>)\n",
      "tensor(8497.) 10000.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "updater = torch.optim.SGD(params, 0.1)\n",
    "num_epochs = 10\n",
    "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    acc = Accumulator(2)\n",
    "    lacc = Accumulator(1)\n",
    "    train_iter = load_data(batch_size, isTrain=True)\n",
    "    test_iter = load_data(batch_size, isTrain=False)\n",
    "    for x, y in train_iter:\n",
    "        updater.zero_grad()\n",
    "        y_hat = net(x)\n",
    "        l = loss(y_hat, y)\n",
    "        lacc.add(l)\n",
    "        l.backward()\n",
    "        updater.step()\n",
    "    print(lacc[0])\n",
    "    for x, y in test_iter:\n",
    "        with torch.no_grad():\n",
    "            y_hat = net(x)\n",
    "            correct_num = (torch.argmax(y_hat, dim=1) == y).type(y.dtype).sum()\n",
    "            acc.add(correct_num, len(y))\n",
    "# #             print(softmax(y_hat))\n",
    "    print(acc[0], acc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c0dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
